"""Micro penetration test API endpoints for PentAGI integration."""

from __future__ import annotations

import asyncio
import os
from typing import Any, Dict, List, Mapping, MutableMapping

from fastapi import APIRouter, Depends, HTTPException, Query, status
from src.api.dependencies import authenticate, authenticated_payload
from src.config.settings import get_settings
import httpx
import structlog

logger = structlog.get_logger(__name__)

router = APIRouter(tags=["micro-pentest"])


def _get_pentagi_client() -> httpx.AsyncClient:
    """Get PentAGI API client."""
    settings = get_settings()
    # Get PentAGI URL from environment or use default
    pentagi_url = os.environ.get("PENTAGI_BASE_URL", "http://pentagi:8443")
    return httpx.AsyncClient(
        base_url=pentagi_url,
        timeout=300.0,  # 5 minutes for pen tests
    )


@router.post("/run", response_model=dict)
async def run_micro_pentest(
    payload: Dict[str, Any] = Depends(authenticated_payload),
    _: None = Depends(authenticate),
) -> MutableMapping[str, Any]:
    """Run micro penetration tests for selected CVEs using PentAGI."""

    cve_ids = payload.get("cve_ids", [])
    target_urls = payload.get("target_urls", [])
    context = payload.get("context", {})

    if not cve_ids:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="At least one CVE ID is required",
        )

    if not target_urls:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="At least one target URL is required",
        )

    logger.info(
        "micro_pentest.start",
        cve_count=len(cve_ids),
        target_count=len(target_urls),
        cve_ids=cve_ids[:5],  # Log first 5 CVEs
    )

    # Prepare PentAGI flow request
    # Based on PentAGI's CreateFlow model structure
    pentagi_payload = {
        "input": f"Perform micro penetration tests for CVEs: {', '.join(cve_ids)}. "
                 f"Target URLs: {', '.join(target_urls)}. "
                 f"Focus on verifying exploitability and impact assessment. "
                 f"Test each CVE individually and provide detailed findings.",
        "provider": "openai",  # Default provider, can be configured via env
        "functions": {
            "disabled": [],
            "functions": [],
        },
    }

    # Add context if provided
    if context:
        pentagi_payload["context"] = context

    try:
        async with _get_pentagi_client() as client:
            # Create PentAGI flow using REST API
            # PentAGI expects: POST /api/v1/flows with CreateFlow model
            response = await client.post(
                "/api/v1/flows",
                json=pentagi_payload,
                headers={
                    "Content-Type": "application/json",
                },
            )

            if response.status_code != status.HTTP_201_CREATED:
                logger.error(
                    "micro_pentest.pentagi_error",
                    status=response.status_code,
                    error=response.text,
                )
                raise HTTPException(
                    status_code=status.HTTP_502_BAD_GATEWAY,
                    detail=f"PentAGI API error: {response.text}",
                )

            # PentAGI returns: { "data": { "id": ..., ... } } or { "id": ..., ... }
            flow_data = response.json()
            flow_obj = flow_data.get("data", flow_data)
            flow_id = flow_obj.get("id") or flow_obj.get("flow_id")

            logger.info("micro_pentest.flow_created", flow_id=flow_id)

            return {
                "status": "started",
                "flow_id": flow_id,
                "cve_ids": cve_ids,
                "target_urls": target_urls,
                "message": f"Micro penetration test started for {len(cve_ids)} CVEs",
            }

    except httpx.TimeoutException:
        logger.error("micro_pentest.timeout")
        raise HTTPException(
            status_code=status.HTTP_504_GATEWAY_TIMEOUT,
            detail="PentAGI request timeout",
        )
    except httpx.RequestError as e:
        logger.error("micro_pentest.request_error", error=str(e))
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail=f"Failed to connect to PentAGI: {str(e)}",
        )


@router.get("/status/{flow_id}", response_model=dict)
async def get_pentest_status(
    flow_id: int,
    _: None = Depends(authenticate),
) -> MutableMapping[str, Any]:
    """Get status of a micro penetration test flow."""

    try:
        async with _get_pentagi_client() as client:
            response = await client.get(
                f"/api/v1/flows/{flow_id}",
            )

            if response.status_code != status.HTTP_200_OK:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to get flow status: {response.text}",
                )

            flow_data = response.json()
            return {
                "flow_id": flow_id,
                "status": flow_data.get("status", "unknown"),
                "progress": flow_data.get("progress", 0),
                "tasks": flow_data.get("tasks", []),
            }

    except httpx.RequestError as e:
        logger.error("micro_pentest.status_error", flow_id=flow_id, error=str(e))
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail=f"Failed to connect to PentAGI: {str(e)}",
        )


@router.post("/batch", response_model=dict)
async def run_batch_micro_pentests(
    payload: Dict[str, Any] = Depends(authenticated_payload),
    _: None = Depends(authenticate),
) -> MutableMapping[str, Any]:
    """Run multiple micro penetration tests in parallel."""

    test_configs = payload.get("test_configs", [])

    if not test_configs:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="At least one test configuration is required",
        )

    logger.info("micro_pentest.batch_start", count=len(test_configs))

    # Run tests in parallel
    tasks = []
    for config in test_configs:
        task_payload = {
            "cve_ids": config.get("cve_ids", []),
            "target_urls": config.get("target_urls", []),
            "context": config.get("context", {}),
        }
        tasks.append(run_micro_pentest(task_payload, None))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful = [r for r in results if not isinstance(r, Exception)]
    failed = [r for r in results if isinstance(r, Exception)]

    return {
        "status": "completed",
        "total": len(test_configs),
        "successful": len(successful),
        "failed": len(failed),
        "results": [
            {
                "status": "success",
                "flow_id": r.get("flow_id"),
                "cve_ids": r.get("cve_ids", []),
            }
            if not isinstance(r, Exception)
            else {"status": "error", "error": str(r)}
            for r in results
        ],
    }


__all__ = ["router"]
