# FixOps Vulnerability Management Gaps Analysis

## Executive Summary

This analysis evaluates FixOps' current capabilities against critical vulnerability management gaps identified in modern security operations: data incompleteness, scanner false positives, zero-day adaptation, and the design-time vs. runtime analysis divide. The analysis is based on code review of the full FixOps codebase.

## Current FixOps Capabilities

### 1. SBOM Handling and Quality Assessment

**Current Implementation:**
- **Location**: `lib4sbom/normalizer.py`, `lib4sbom/quality.py`
- **Capabilities**:
  - Multi-format SBOM normalization (CycloneDX, SPDX)
  - Component deduplication and merging
  - Quality metrics: coverage, license coverage, resolvability, generator variance
  - Validation error detection for missing required fields (name, version, purl)
  - Policy status evaluation (pass/warn based on coverage thresholds)

**Gap Analysis:**
- ✅ **Addresses**: SBOM quality detection and normalization
- ⚠️ **Partial**: Quality metrics exist but don't specifically flag the "70% lack detail" problem
- ❌ **Missing**: No automated enrichment of incomplete SBOMs with external data sources
- ❌ **Missing**: No integration with NTIA SBOM quality standards or scoring

**Evidence from Code:**
```python
# lib4sbom/normalizer.py:300-316
missing_fields = [
    field
    for field, value in (
        ("name", display_name),
        ("version", normalized_version),
        ("purl", display_purl),
    )
    if not value
]
if missing_fields:
    validation_errors.append({
        "path": str(path),
        "generator": generators,
        "missing_fields": missing_fields,
    })
```

### 2. KEV/EPSS Integration

**Current Implementation:**
- **Location**: `risk/enrichment.py`, `core/exploit_signals.py`, `risk/scoring.py`
- **Capabilities**:
  - KEV feed integration with auto-refresh (24h interval)
  - EPSS score integration
  - ExploitDB reference counting
  - CVSS extraction and scoring
  - CWE identification
  - Age calculation (days since publication)
  - Vendor advisory detection

**Gap Analysis:**
- ✅ **Addresses**: KEV/EPSS integration exists
- ⚠️ **Partial**: Auto-refresh exists but doesn't address the "KEV lags by weeks" problem
- ❌ **Missing**: No continuous threat intelligence feeds beyond KEV/EPSS
- ❌ **Missing**: No zero-day detection mechanisms (pre-KEV publication)
- ❌ **Missing**: No integration with additional threat feeds (e.g., GitHub Security Advisories, OSV)

**Evidence from Code:**
```python
# core/exploit_signals.py:256-260
self.refresh_interval_hours = (
    int(refresh.get("refresh_interval_hours", 24))
    if isinstance(refresh, Mapping)
    else 24
)
```

### 3. Risk Scoring and Prioritization

**Current Implementation:**
- **Location**: `risk/scoring.py`, `risk/forecasting.py`
- **Capabilities**:
  - Composite risk scoring using EPSS, KEV, version lag, and exposure
  - Weighted scoring model (default: EPSS 50%, KEV 20%, version lag 20%, exposure 10%)
  - Exposure-based risk adjustment (internet/public/partner/internal/controlled)
  - Version lag calculation (days behind patched version)
  - Probabilistic forecasting (Bayesian + Markov models)
  - 30-day exploitation probability forecasting

**Gap Analysis:**
- ✅ **Addresses**: Multi-factor risk scoring with EPSS/KEV
- ⚠️ **Partial**: Scoring exists but doesn't specifically address false positive reduction
- ❌ **Missing**: No reachability analysis (Endor Labs-style code analysis)
- ❌ **Missing**: No data-flow tracing to verify exploitability
- ❌ **Missing**: No distinction between invoked vs. non-invoked code paths

**Evidence from Code:**
```python
# risk/scoring.py:213-270
def _score_vulnerability(
    component: Mapping[str, Any],
    vulnerability: Mapping[str, Any],
    epss_scores: Mapping[str, float],
    kev_entries: Mapping[str, Any],
    weights: Mapping[str, float],
) -> Dict[str, Any] | None:
    # ... scoring logic using EPSS, KEV, version lag, exposure
    # No code analysis or reachability checks
```

### 4. Design-Time vs. Runtime Analysis

**Current Implementation:**
- **Location**: `core/context_engine.py`, `apps/api/pipeline.py`, `services/graph/graph.py`
- **Capabilities**:
  - Design-time component correlation (design CSV → SBOM → SARIF → CVE)
  - Business context enrichment (criticality, data classification, exposure)
  - Knowledge graph construction (provenance, dependencies, relationships)
  - Crosswalk construction linking design components to technical artifacts

**Gap Analysis:**
- ✅ **Addresses**: Design-time logical context correlation
- ❌ **Critical Gap**: No runtime code analysis
- ❌ **Missing**: No static code analysis for exploitability verification
- ❌ **Missing**: No call graph analysis to identify invoked code paths
- ❌ **Missing**: No data-flow analysis to trace vulnerability exposure
- ❌ **Missing**: No distinction between design-time assumptions and runtime reality

**Evidence from Code:**
```python
# apps/api/pipeline.py (crosswalk construction)
# Links design → SBOM → findings → CVEs based on token matching
# No code analysis or runtime verification
```

### 5. Triage and False Positive Reduction

**Current Implementation:**
- **Location**: `core/context_engine.py`, `risk/scoring.py`
- **Capabilities**:
  - Context-aware severity adjustment
  - Business context scoring (criticality, data classification, exposure)
  - Playbook-based response recommendations
  - Guardrail evaluation

**Gap Analysis:**
- ⚠️ **Partial**: Context-aware scoring exists but not specifically for false positive reduction
- ❌ **Missing**: No explicit triage workflow for human-vetted review
- ❌ **Missing**: No false positive tracking or metrics (MTTR, false-negative rates)
- ❌ **Missing**: No noise reduction mechanisms beyond EPSS/KEV filtering

**Evidence from Code:**
- No dedicated triage module found
- No false positive tracking or metrics collection

### 6. Policy-as-Code

**Current Implementation:**
- **Location**: `policy/*.rego`, `policy/psl/bundle.psl`
- **Capabilities**:
  - OPA (Open Policy Agent) Rego policies for infrastructure
  - Policy evaluation for security controls (database encryption, network rules, etc.)
  - Policy automation via `core/policy.py`

**Gap Analysis:**
- ✅ **Addresses**: Policy-as-code for infrastructure/configs
- ⚠️ **Partial**: Policies exist but no explicit "analyst review for uncertain cases" workflow
- ❌ **Missing**: No policy-as-code for vulnerability triage decisions
- ❌ **Missing**: No integration with vulnerability-specific policies (e.g., "require analyst review if EPSS > 0.5 but no KEV")

**Evidence from Code:**
```rego
# policy/APP1/security_controls.rego
deny[msg] {
  some change
  change := input.resource_changes[_]
  change.change.after.type == "aws_db_instance"
  change.change.after.publicly_accessible
  msg := "Database instances must not be publicly accessible."
}
```

## Critical Gaps Identified

### Gap 1: Runtime Code Analysis and Reachability

**Problem**: FixOps relies solely on design-time logical context, which risks inaccuracies as systems evolve. Per NIST's 2024 SSDF, 60% of vulnerabilities stem from post-design changes.

**Current State**: 
- No static code analysis
- No call graph construction
- No data-flow tracing
- No reachability analysis (Endor Labs-style)

**Impact**: 
- Over-prioritization of non-invoked code vulnerabilities
- Missing actual runtime exposures
- False positives from design-time assumptions

**Recommendation**:
1. Integrate static analysis tools (e.g., CodeQL, Semgrep) for call graph construction
2. Implement reachability analysis to identify if vulnerable code is actually invoked
3. Add data-flow tracing to verify exploitability paths
4. Distinguish between "vulnerable component present" vs. "vulnerable code path reachable"

### Gap 2: Zero-Day and Continuous Threat Intelligence

**Problem**: KEV lags by weeks per CISA, leaving zero-days undetected.

**Current State**:
- KEV/EPSS integration exists with 24h refresh
- No pre-KEV zero-day detection
- Limited threat intelligence sources

**Impact**:
- Zero-days remain undetected until KEV publication
- Delayed response to emerging threats

**Recommendation**:
1. Integrate additional threat feeds (GitHub Security Advisories, OSV, vendor advisories)
2. Implement anomaly detection for unusual vulnerability patterns
3. Add social media/security researcher feed monitoring
4. Create early-warning system for pre-KEV threats

### Gap 3: False Positive Reduction and Triage Workflow

**Problem**: Scanner false positives up to 40% noise per Picus Security. Need 95% noise reduction via hybrid systems.

**Current State**:
- EPSS/KEV filtering exists
- No explicit triage workflow
- No false positive tracking

**Impact**:
- Analyst fatigue from false positives
- Inefficient resource allocation

**Recommendation**:
1. Implement explicit triage workflow with human-vetted review gates
2. Add false positive tracking and metrics (MTTR, false-negative rates)
3. Create automated triage rules (e.g., "auto-dismiss if EPSS < 0.1 and not KEV")
4. Implement confidence scoring for automated decisions
5. Add "uncertain case" flagging for analyst review

### Gap 4: SBOM Completeness and Enrichment

**Problem**: 70% of SBOMs lack detail per 2023 NTIA.

**Current State**:
- SBOM quality metrics exist
- Validation error detection
- No automated enrichment

**Impact**:
- Incomplete vulnerability coverage
- Missing transitive dependencies

**Recommendation**:
1. Integrate with package registries (npm, PyPI, Maven) for missing metadata
2. Implement transitive dependency resolution
3. Add SBOM enrichment from multiple sources
4. Create SBOM quality scoring aligned with NTIA standards

### Gap 5: Design-Time vs. Runtime Verification

**Problem**: Logical assumptions miss runtime realities, such as unintended exposures.

**Current State**:
- Design-time correlation exists
- No runtime verification
- No distinction between design assumptions and runtime reality

**Impact**:
- False negatives from design-time assumptions
- Missing actual runtime exposures

**Recommendation**:
1. Integrate runtime security tools (e.g., Falco, eBPF-based monitoring)
2. Add runtime vulnerability scanning (container scanning, runtime SBOM)
3. Compare design-time assumptions with runtime observations
4. Flag discrepancies between design and runtime

## Recommended Implementation Roadmap

### Phase 1: Immediate Improvements (1-2 months)

1. **Enhanced Triage Workflow**
   - Add triage module with review gates
   - Implement confidence scoring
   - Create "uncertain case" flagging

2. **False Positive Tracking**
   - Add metrics collection (MTTR, false-negative rates)
   - Implement feedback loop for triage decisions
   - Create dashboard for noise reduction metrics

3. **SBOM Enrichment**
   - Integrate package registry APIs
   - Implement transitive dependency resolution
   - Add NTIA-aligned quality scoring

### Phase 2: Code Analysis Integration (3-4 months)

1. **Static Analysis Integration**
   - Integrate CodeQL or Semgrep
   - Implement call graph construction
   - Add reachability analysis

2. **Data-Flow Tracing**
   - Implement taint analysis
   - Add exploitability path verification
   - Distinguish invoked vs. non-invoked code

3. **Hybrid Scoring Model**
   - Combine design-time context with code analysis results
   - Weight reachability analysis in risk scoring
   - Reduce false positives from non-invoked code

### Phase 3: Advanced Threat Intelligence (4-6 months)

1. **Multi-Source Threat Feeds**
   - Integrate GitHub Security Advisories
   - Add OSV database integration
   - Implement vendor advisory feeds

2. **Zero-Day Detection**
   - Add anomaly detection for unusual patterns
   - Implement early-warning system
   - Create pre-KEV threat monitoring

3. **Continuous Threat Intelligence**
   - Real-time feed updates
   - Social media monitoring
   - Security researcher feed integration

### Phase 4: Runtime Verification (6-8 months)

1. **Runtime Security Integration**
   - Integrate Falco or eBPF-based monitoring
   - Add container runtime scanning
   - Implement runtime SBOM generation

2. **Design vs. Runtime Comparison**
   - Compare design-time assumptions with runtime observations
   - Flag discrepancies
   - Update risk scores based on runtime data

## Metrics and KPIs

### Current Metrics (Available)
- Component risk scores
- CVE counts
- EPSS/KEV match rates
- SBOM quality metrics

### Recommended Additional Metrics
1. **MTTR (Mean Time to Remediate)**: Track time from detection to fix
2. **False Positive Rate**: Percentage of dismissed vulnerabilities
3. **False Negative Rate**: Missed vulnerabilities discovered later
4. **Noise Reduction**: Percentage reduction in alerts after filtering
5. **Reachability Coverage**: Percentage of vulnerabilities with reachability analysis
6. **Zero-Day Detection Time**: Time from zero-day to detection
7. **SBOM Completeness**: Percentage of components with complete metadata

## Conclusion

FixOps has a solid foundation with EPSS/KEV integration, risk scoring, and design-time correlation. However, critical gaps exist in:

1. **Runtime code analysis** (reachability, data-flow tracing)
2. **Zero-day detection** (pre-KEV threat intelligence)
3. **False positive reduction** (explicit triage workflows, metrics)
4. **SBOM enrichment** (automated completeness improvement)
5. **Runtime verification** (design vs. runtime comparison)

The recommended roadmap addresses these gaps systematically, moving from immediate improvements (triage, metrics) to advanced capabilities (code analysis, zero-day detection, runtime verification). This aligns with the Endor Labs approach of combining EPSS/KEV with human-vetted triage and code analysis for 95% noise reduction, while adding runtime verification to address the design-time vs. runtime divide.

## References

- NIST SSDF 2024: 60% of vulnerabilities from post-design changes
- NTIA 2023: 70% of SBOMs lack detail
- Picus Security: Up to 40% scanner false positives
- CISA: KEV lags by weeks for zero-days
- Endor Labs: Reachability analysis cuts noise by 90% beyond CVSS/KEV
